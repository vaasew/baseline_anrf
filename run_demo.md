# Demo Run Instructions

This guide explains **exactly how to set up and run a demo pipeline** from -
dataset preparation â†’ training â†’ inference â†’ evaluation.

If a **GPU is available**, the **entire pipeline finishes in under ~30 minutes**.

---

#  Important Notes

*  The demo uses **only certain features from April 2016 data**
*  Training epochs is set to **50 epochs**
*  The experiment name is fixed as: **`demo`**
*  All paths and configs are already set in the repo

---

# 1ï¸âƒ£ Clone the Repository (run_demo branch)

```bash
git clone -b run_demo https://github.com/vaasew/baseline_anrf.git
cd baseline_anrf
```

Make sure you are on the correct branch:

```bash
git branch
# should show: * run_demo
```

---

# 2ï¸âƒ£ Download Demo Dataset

Download the data from:

ğŸ‘‰ [https://drive.google.com/file/d/1kajlQ_FVpaZjoUSdFV_dyd0TVHtcQdpu/view?usp=sharing](https://drive.google.com/file/d/1kajlQ_FVpaZjoUSdFV_dyd0TVHtcQdpu/view?usp=sharing)

The data contains:

* Features (April 2016 only):

  * `pm25`
  * `t2`
  * `rainc`
  * `rainnc`
  * `PM25_e`
  * `PM25_f`

  stored in the form - 
  `APRIL_2016_HOURLY_<feature_name>.npy`

* Normalization file(for pre-processing):

  * `min_max.mat`

---

# 3ï¸âƒ£ Place Files in Correct Directories

### ğŸ”¹ Feature files

Place all features of the form -

```
APRIL_2016_HOURLY_<feature_name>.npy
```

inside:

```
data/raw/
```

Example:

```
data/raw/APRIL_2016_HOURLY_pm25.npy
data/raw/APRIL_2016_HOURLY_t2.npy
...
```

---

### ğŸ”¹ Normalization file

Place `min_max.mat` inside:

```
data/stats/
```

Final structure should look like:

```
data/
 â”œâ”€â”€ raw/
 â”‚    â”œâ”€â”€ APRIL_2016_HOURLY_pm25.npy
 â”‚    â”œâ”€â”€ APRIL_2016_HOURLY_t2.npy
 â”‚    â”œâ”€â”€ ...
 â””â”€â”€ stats/
      â””â”€â”€ min_max.mat
```

---
# 4ï¸âƒ£ Run the Demo Pipeline

Before running the demo, **ensure your environment is properly set up**
(required packages installed, correct Python environment activated, CUDA configured if using a GPU, paths updated if needed).

Both options below execute the **entire pipeline end-to-end**.

---

## âœ… Option A: Run locally

Activate your environment, then run:

```bash
bash run.sh
```

---

## âœ… Option B: Run on an HPC cluster

Submit the job script:

```bash
qsub run_job.pbs
```

Before submitting, **edit `run_job.pbs` as per your system setup**
(module loads, environment activation, paths, GPU settings, etc.).
The provided file is only a **boilerplate template**.

---




# 5ï¸âƒ£ What the Pipeline Does

Running `run.sh` or `run_job.pbs` will automatically execute the following stages:

**dataset preparation â†’ training â†’ inference â†’ evaluation**

---

## ğŸ”¹ (1) Dataset preparation

Runs:

```
prepare_dataset.py
```

Using:

* Configuration from:

  ```
  prepare_dataset.yaml
  ```
* Raw feature files from:

  ```
  data/raw/
  ```

Creates time-series samples and stores **training and validation sets for each feature** in:

```
data/met/
data/emissions/
```

Each directory contains:

```
train_<feature_name>.npy
val_<feature_name>.npy
```

---

## ğŸ”¹ (2) Model training

Runs:

```
train.py
```

Using:

* Configuration from:

  ```
  train.yaml
  ```
* Training and validation samples from:

  ```
  data/met/
  data/emissions/
  ```

Outputs:

* Model checkpoints:

  ```
  experiments/demo/checkpoints/*.pt
  ```
* Training logs:

  ```
  experiments/demo/logs/
  ```

---

## ğŸ”¹ (3) Inference on validation set

Runs:

```
infer.py
```

Using:

* Configuration from:

  ```
  infer.yaml
  ```
* Trained model checkpoint:

  ```
  experiments/demo/checkpoints/demo_model_ep49.pt
  ```

Outputs:

```
experiments/demo/infer/val.npy
```

(Stored validation set predictions)

---

## ğŸ”¹ (4) Evaluation

Runs:

```
eval.py
```

Using:

* Configuration from:

  ```
  eval.yaml
  ```
* Model predictions on the validation set (first 10 hours):

  ```
  experiments/demo/infer/val.npy
  ```
* Reference validation targets (last 16 hours):

  ```
  data/met/val_pm25.npy
  ```

This step computes error metrics and generates the final evaluation CSV files.

---

### âœ”ï¸ Whole-domain metrics

```
experiments/demo/eval_results/val_domain.csv
```

### âœ”ï¸ City-wise metrics

```
experiments/demo/eval_results/cities/*.csv
```

Each city has a different csv. Ex-

```
experiments/demo/eval_results/cities/delhi.csv
```

---

# Demo Run Complete

After successful execution, you should have:

* trained model checkpoints
* validation predictions
* domain-level evaluation CSV
* city-wise evaluation CSVs

inside:

```
experiments/demo/
```

---
